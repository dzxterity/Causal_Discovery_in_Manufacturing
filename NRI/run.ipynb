{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bccb8578",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, cuda=True, decoder='mlp', decoder_dropout=0.0, decoder_hidden=256, dims=4, dynamic_graph=False, edge_types=2, encoder='mlp', encoder_dropout=0.0, encoder_hidden=256, epochs=5, factor=True, gamma=0.5, hard=False, load_folder='', lr=0.0005, lr_decay=200, no_cuda=False, no_factor=False, num_atoms=5, prediction_steps=10, prior=False, save_folder='logs', seed=42, skip_first=False, suffix='_springs5', temp=0.5, timesteps=49, var=5e-05)\n",
      "/home/jovyan/NRI-master/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using factor graph MLP encoder.\n",
      "/home/jovyan/NRI-master/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using learned interaction net decoder.\n",
      "/home/jovyan/.imgenv-baseline-0/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/jovyan/NRI-master/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n",
      "train.py:249: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, relations = Variable(data, volatile=True), Variable(\n",
      "train.py:250: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  relations, volatile=True)\n",
      "Epoch: 0000 nll_train: 1307.5848816386 kl_train: -0.5165368876 mse_train: 0.0006810338 acc_train: 0.5122216672 nll_val: 17.9865401546 kl_val: -0.7426830734 mse_val: 0.0000093680 acc_val: 0.5713953718 time: 36.7820s\n",
      "Best model so far, saving...\n",
      "Epoch: 0001 nll_train: 444.6402107102 kl_train: -0.5994469030 mse_train: 0.0002315835 acc_train: 0.7141072570 nll_val: 10.0584109343 kl_val: -0.4221819688 mse_val: 0.0000052388 acc_val: 0.8847260680 time: 34.3500s\n",
      "Best model so far, saving...\n",
      "Epoch: 0002 nll_train: 163.4080697569 kl_train: -0.2507450435 mse_train: 0.0000851084 acc_train: 0.9331529731 nll_val: 4.7468213975 kl_val: -0.1970817596 mse_val: 0.0000024723 acc_val: 0.9443680775 time: 34.3166s\n",
      "Best model so far, saving...\n",
      "Epoch: 0003 nll_train: 107.3827316206 kl_train: -0.1548539500 mse_train: 0.0000559285 acc_train: 0.9533621723 nll_val: 3.1648223249 kl_val: -0.1335952309 mse_val: 0.0000016483 acc_val: 0.9580399525 time: 34.1086s\n",
      "Best model so far, saving...\n",
      "Epoch: 0004 nll_train: 87.0081270028 kl_train: -0.1184627578 mse_train: 0.0000453167 acc_train: 0.9615053549 nll_val: 2.8351276419 kl_val: -0.1098914127 mse_val: 0.0000014766 acc_val: 0.9614319620 time: 35.1231s\n",
      "Best model so far, saving...\n",
      "Optimization Finished!\n",
      "Best Epoch: 0004\n",
      "train.py:313: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, relations = Variable(data, volatile=True), Variable(\n",
      "train.py:314: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  relations, volatile=True)\n",
      "--------------------------------\n",
      "--------Testing-----------------\n",
      "--------------------------------\n",
      "nll_test: 83.0836789819 kl_test: -0.1094414003 mse_test: 0.0000432728 acc_test: 0.9629895174\n",
      "MSE: [ 0.000001851206 , 0.000007452966 , 0.000016861579 , 0.000030110423 , 0.000047212154 , 0.000068161615 , 0.000092937436 , 0.000121501842 , 0.000153806599 , 0.000189795668 , 0.000229403231 , 0.000272558129 , 0.000319181534 , 0.000369198620 , 0.000422529265 , 0.000479103648 , 0.000538856315 , 0.000604006869 , 0.000672373048 , 0.000743778131 ]\n",
      "logs/exp2023-07-24T10:27:08.443431/\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e3f312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9189fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP encoder.\n",
      "Using learned interaction net decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/NRI-master/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "/home/jovyan/NRI-master/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "#default\n",
    "timesteps = 49\n",
    "dims = 4\n",
    "encoder_hidden = 256\n",
    "edge_types = 2\n",
    "encoder_dropout = 0.0\n",
    "factor = False \n",
    "decoder_hidden = 256\n",
    "decoder_dropout = 0.0\n",
    "skip_first = False\n",
    "\n",
    "encoder = MLPEncoder(timesteps * dims, encoder_hidden,\n",
    "                         edge_types,\n",
    "                         encoder_dropout, factor)\n",
    "decoder = MLPDecoder(n_in_node=dims,\n",
    "                         edge_types=edge_types,\n",
    "                         msg_hid=decoder_hidden,\n",
    "                         msg_out=decoder_hidden,\n",
    "                         n_hid=decoder_hidden,\n",
    "                         do_prob=decoder_dropout,\n",
    "                         skip_first=skip_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e853779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "encoder_file = os.path.join('logs/exp2023-07-24T10:27:08.443431', 'encoder.pt')\n",
    "decoder_file = os.path.join('logs/exp2023-07-24T10:27:08.443431', 'decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d3a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847e25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('data/train_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adafadfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45aa21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d457d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load_data(batch_size=1, suffix=''):\n",
    "    \n",
    "    train = np.load('data/train_small.npy')\n",
    "    valid = np.load('data/test_small.npy')\n",
    "\n",
    "\n",
    "    # [num_samples, num_timesteps, num_dims, num_atoms]\n",
    "    num_atoms = train.shape[3]\n",
    "\n",
    "    loc_max = -1\n",
    "    loc_min = -1\n",
    "    vel_max = -1\n",
    "    vel_min = -1\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    #loc_train = (loc_train - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    #vel_train = (vel_train - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    #loc_valid = (loc_valid - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    #vel_valid = (vel_valid - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    #loc_test = (loc_test - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    #vel_test = (vel_test - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    # Reshape to: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "    feat_train = np.transpose(train, [0, 3, 1, 2])\n",
    "    feat_valid = np.transpose(valid, [0, 3, 1, 2])\n",
    "\n",
    "    feat_train = torch.FloatTensor(feat_train)\n",
    "    feat_valid = torch.FloatTensor(feat_valid)\n",
    "\n",
    "    # Exclude self edges\n",
    "    off_diag_idx = np.ravel_multi_index(\n",
    "        np.where(np.ones((num_atoms, num_atoms)) - np.eye(num_atoms)),\n",
    "        [num_atoms, num_atoms])\n",
    "\n",
    "    edges_train = feat_train # мусор\n",
    "    edges_valid = feat_valid # мусор\n",
    "    \n",
    "    train_data = TensorDataset(feat_train, edges_train)\n",
    "    valid_data = TensorDataset(feat_valid, edges_valid)\n",
    "\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    valid_data_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "    \n",
    "    test_data_loader = DataLoader(valid_data, batch_size=batch_size) # фиктивная строка\n",
    "\n",
    "    return train_data_loader, valid_data_loader, test_data_loader, loc_max, loc_min, vel_max, vel_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a4526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, loc_max, loc_min, vel_max, vel_min = my_load_data(\n",
    "    50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5f9ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5e9a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2a791",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=50, cuda=True, decoder='mlp', decoder_dropout=0.0, decoder_hidden=128, dims=1, dynamic_graph=False, edge_types=2, encoder='mlp', encoder_dropout=0.0, encoder_hidden=128, epochs=100, factor=True, gamma=0.5, hard=False, load_folder='logs/exp2023-07-26T16:27:04.767683', lr=0.0005, lr_decay=200, no_cuda=False, no_factor=False, num_atoms=52, prediction_steps=10, prior=False, save_folder='logs', seed=42, skip_first=False, suffix='_springs5', temp=0.5, timesteps=30, var=5e-05)\n",
      "/home/jovyan/NRI-master/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using factor graph MLP encoder.\n",
      "/home/jovyan/NRI-master/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using learned interaction net decoder.\n",
      "/home/jovyan/.imgenv-baseline-0/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/jovyan/NRI-master/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n",
      "train-my.py:250: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n",
      "Epoch: 0000 nll_train: 157480.9909248737 kl_train: -5.8017759997 mse_train: 0.5430378860 nll_val: 73137.1999023438 kl_val: -4.7494488716 mse_val: 0.2521972355 time: 215.4286s\n",
      "Best model so far, saving...\n",
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch: 0001 nll_train: 91078.9875315657 kl_train: -5.0361990014 mse_train: 0.3140654681 nll_val: 74387.8787109375 kl_val: -5.5902956247 mse_val: 0.2565099172 time: 210.9171s\n"
     ]
    }
   ],
   "source": [
    "!python train-my.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7e4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a296c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=50, cuda=True, decoder='mlp', decoder_dropout=0.0, decoder_hidden=128, dims=1, dynamic_graph=False, edge_types=2, encoder='mlp', encoder_dropout=0.0, encoder_hidden=128, epochs=100, factor=True, gamma=0.5, hard=False, load_folder='', lr=0.0005, lr_decay=200, no_cuda=False, no_factor=False, num_atoms=52, prediction_steps=10, prior=False, save_folder='logs', seed=42, skip_first=False, suffix='_springs5', temp=0.5, timesteps=30, var=5e-05)\n",
      "/home/jovyan/NRI-master/modules.py:27: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using factor graph MLP encoder.\n",
      "/home/jovyan/NRI-master/modules.py:110: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(m.weight.data)\n",
      "Using learned interaction net decoder.\n",
      "/home/jovyan/.imgenv-baseline-0/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/jovyan/NRI-master/utils.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n",
      "train-my.py:250: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n",
      "Epoch: 0000 nll_train: 4316412910.4139509201 kl_train: -8.6934103421 mse_train: 14884.1820666139 nll_val: 77961.7921875000 kl_val: -9.4902663946 mse_val: 0.2688337646 time: 198.0120s\n",
      "Best model so far, saving...\n",
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Epoch: 0001 nll_train: 125225.8180803571 kl_train: -8.2522493124 mse_train: 0.4318131543 nll_val: 77276.0051757812 kl_val: -9.4902985096 mse_val: 0.2664689787 time: 195.0944s\n",
      "Best model so far, saving...\n",
      "Epoch: 0002 nll_train: 105338.2007812500 kl_train: -8.2522491830 mse_train: 0.3632351641 nll_val: 76116.2245117188 kl_val: -9.4903035641 mse_val: 0.2624697350 time: 197.0087s\n",
      "Best model so far, saving...\n",
      "Epoch: 0003 nll_train: 103950.7736049107 kl_train: -8.2522464480 mse_train: 0.3584509334 nll_val: 75670.1268554688 kl_val: -9.4903117895 mse_val: 0.2609314654 time: 200.7422s\n",
      "Best model so far, saving...\n",
      "Epoch: 0004 nll_train: 102993.7030691964 kl_train: -8.2522449459 mse_train: 0.3551506907 nll_val: 75165.7664062500 kl_val: -9.4903228521 mse_val: 0.2591922916 time: 198.4357s\n",
      "Best model so far, saving...\n"
     ]
    }
   ],
   "source": [
    "!python train-my.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba91d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': Namespace(batch_size=50, cuda=True, decoder='mlp', decoder_dropout=0.0, decoder_hidden=128, dims=1, dynamic_graph=False, edge_types=2, encoder='mlp', encoder_dropout=0.0, encoder_hidden=128, epochs=100, factor=True, gamma=0.5, hard=False, load_folder='logs/exp2023-07-26T16:27:04.767683', lr=0.0005, lr_decay=200, no_cuda=False, no_factor=False, num_atoms=52, prediction_steps=10, prior=False, save_folder='logs', seed=42, skip_first=False, suffix='_springs5', temp=0.5, timesteps=30, var=5e-05)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "with open('logs/exp2023-07-27T01:09:21.423069/metadata.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffbccfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81887864",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'old_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_200/3174760033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mold_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'old_dict' is not defined"
     ]
    }
   ],
   "source": [
    "new_dict = OrderedDict([(key.replace('model.', ''), value) for key, value in old_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee125a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
